{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Program to measure similarity between  \n",
    "# two sentences using cosine similarity. \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from collections import Counter\n",
    "import math \n",
    "\n",
    "class QueryLikelihood(object):\n",
    "    \n",
    "    def JMScore(self, Query, Doc, lmbda):\n",
    "  \n",
    "        # tokenization \n",
    "        X_list = word_tokenize(Query)  \n",
    "        Y_list = word_tokenize(Doc) \n",
    "\n",
    "        # sw contains the list of stopwords \n",
    "        sw = stopwords.words('english')   \n",
    "\n",
    "        # remove stop words from string \n",
    "        X_set = {w for w in X_list if not w in sw}  \n",
    "        Y_set = {w for w in Y_list if not w in sw} \n",
    "        \n",
    "        #Length of Doc\n",
    "        Doc_length = len(Y_set)\n",
    "        #print('doc length',Doc_length)\n",
    "\n",
    "        # form a set containing keywords of both strings  \n",
    "        rvector = X_set.union(Y_set)\n",
    "        #print(rvector)\n",
    "        \n",
    "        Querry_TF_Vector = [0] * len(rvector)\n",
    "        Doc_TF_Vector = [0] * len(rvector)\n",
    "        \n",
    "        #trace the index in rvector\n",
    "        i = 0\n",
    "        for w in rvector: \n",
    "            if w in X_set: \n",
    "                 Querry_TF_Vector[i] = X_list.count(w) # create a vector  \n",
    "            if w in Y_set: \n",
    "                Doc_TF_Vector[i] = Y_list.count(w)\n",
    "            i = i+1\n",
    "                \n",
    "\n",
    "            \n",
    "        JM_score = 0\n",
    "        #print( Querry_TF_Vector)\n",
    "        #print(Doc_TF_Vector)\n",
    "\n",
    "        # JM Score formula \n",
    "        \n",
    "        #P_REF = 0.01\n",
    "        P_REF = 0.01\n",
    "        \n",
    "        for i in range(len(rvector)): \n",
    "                l_numerator = (1 - lmbda) * Doc_TF_Vector[i]\n",
    "                l_denominator = lmbda * P_REF * Doc_length\n",
    "                l_frac = l_numerator/l_denominator\n",
    "                JM_score +=  Querry_TF_Vector[i]*math.log(1 + l_frac)\n",
    "        \n",
    "        #print(\"JM Score: \", JM_score)\n",
    "        return JM_score\n",
    "        \n",
    "    def DPScore(self, Query, Doc, mu):\n",
    "\n",
    "            # tokenization \n",
    "            X_list = word_tokenize(Query)  \n",
    "            Y_list = word_tokenize(Doc) \n",
    "\n",
    "            # sw contains the list of stopwords \n",
    "            sw = stopwords.words('english')   \n",
    "\n",
    "            # remove stop words from string \n",
    "            X_set = {w for w in X_list if not w in sw}  \n",
    "            Y_set = {w for w in Y_list if not w in sw} \n",
    "\n",
    "            #Length of Doc and Query\n",
    "            Query_length = len(X_set)\n",
    "            Doc_length = len(Y_set)\n",
    "            #print('doc length',Doc_length)\n",
    "\n",
    "            # form a set containing keywords of both strings  \n",
    "            rvector = X_set.union(Y_set)\n",
    "            #print(rvector)\n",
    "\n",
    "            Querry_TF_Vector = [0] * len(rvector)\n",
    "            Doc_TF_Vector = [0] * len(rvector)\n",
    "\n",
    "            #trace the index in rvector\n",
    "            i = 0\n",
    "            for w in rvector: \n",
    "                if w in X_set: \n",
    "                     Querry_TF_Vector[i] = X_list.count(w) # create a vector  \n",
    "                if w in Y_set: \n",
    "                    Doc_TF_Vector[i] = Y_list.count(w)\n",
    "                i = i+1\n",
    "\n",
    "\n",
    "\n",
    "            DP_score = 0\n",
    "            #print( Querry_TF_Vector)\n",
    "            #print(Doc_TF_Vector)\n",
    "\n",
    "            # JM Score formula \n",
    "\n",
    "            #P_REF = 0.01\n",
    "            P_REF = 0.01\n",
    "\n",
    "            for i in range(len(rvector)): \n",
    "                    l_numerator = Doc_TF_Vector[i]\n",
    "                    l_denominator = mu * P_REF\n",
    "                    l_frac = l_numerator/l_denominator\n",
    "                    doc_norm = Query_length * math.log(Doc_length + mu)\n",
    "                    DP_score +=  Querry_TF_Vector[i]*math.log(1 + l_frac) - doc_norm \n",
    "\n",
    "            #print(\"DP Score: \", DP_score)\n",
    "            return DP_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
